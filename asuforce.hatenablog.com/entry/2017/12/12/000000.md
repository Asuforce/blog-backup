---
Title: ファイルの分散配布に挑戦した tuggle 完結編
Category:
- tech
Date: 2017-12-12T00:00:00+09:00
URL: https://asuforce.hatenablog.com/entry/2017/12/12/000000
EditURL: https://blog.hatena.ne.jp/asuforcegt/asuforce.hatenablog.com/atom/entry/8599973812325662906
---

<p><a href="https://qiita.com/advent-calendar/2017/pepabo">GMOペパボ Advent Calendar 2017</a>の12日分のエントリーになります。</p>
<p> </p>
<p>どうも！本日は分散デプロイについての記事になります。2年目氏のスンがお送りいたします。</p>
<p>今年最大のバリューは自分の担当サービスに分散デプロイを導入したことだと思うので、ふりかえりも兼ねて紹介します。書こうと思っていたのですが、なかなか筆が進まなかったです。デプロイを高速化したいという方にぜひご覧いただきたいです。</p>
<h4>挫折</h4>
<p>僕の上期の目標は、デプロイの高速化でした。下記の資料をご覧いただくとわかるのですが、色々挑戦した後、ファイルの分散配布への挑戦を行い挫折しました。その時導入を検討したのが <a href="https://twitter.com/fujiwara">@fujiwara</a> さんが作成した、 <a href="https://github.com/fujiwara/tuggle">tuggle</a> というプロダクトでした。</p>
<p> </p>
<script src="http://speakerdeck.com/embed/768886a9cfb14ee7a5267768f1cb99c7.js"></script>
<p> </p>
<p>資料をご覧いただくとわかるように、プライベートクラウドの母艦のネットワーク負荷が上がり、ファイルの配布をするにつれて帯域を圧迫する問題が発生しました。その結果、配布待ちのノードがリトライを諦め、次々に fail するという事態になりました。この時は十分な対策ができなかったので導入は見送りました。</p>
<h5>tuggle とは</h5>
<p><iframe class="embed-card embed-blogcard" style="display: block; width: 100%; height: 190px; max-width: 500px; margin: 10px 0px;" title="Consulクラスタ内でファイルを分散配布する tuggle を書いた - 酒日記 はてな支店" src="https://hatenablog-parts.com/embed?url=http%3A%2F%2Fsfujiwara.hatenablog.com%2Fentry%2F2017%2F02%2F14%2F090000" frameborder="0" scrolling="no"></iframe><cite class="hatena-citation"><a href="http://sfujiwara.hatenablog.com/entry/2017/02/14/090000">sfujiwara.hatenablog.com</a></cite></p>
<p> </p>
<p>この記事に書いてあるように、 consul クラスタ内での分散配布を実現するプロダクトです。<a href="https://github.com/fujiwara/stretcher">stretcher</a> の src に tuggle のエンドポイントを指定すると、親ノードがファイルを配布。ファイルを取得したノードは、取得後に配布ノードに切り替わるという便利プロダクトになります。</p>
<h4>fujiwara さんの協力</h4>
<blockquote class="twitter-tweet" data-lang="ja">
<p dir="ltr" lang="ja">しかし1.4GBのtarball毎回配布はなかなか厳しそうだな…</p>
— fujiwara (@fujiwara) <a href="https://twitter.com/fujiwara/status/867992168354283521?ref_src=twsrc%5Etfw">2017年5月26日</a></blockquote>
<p>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</p>
<p>上記のスライドを Twitter で共有したところ、 fujiwara さんからレスポンスをいただき、更に fetch-timeout というオプションを追加していただきました。リトライのデフォルトが10分になりました。</p>
<p><iframe class="embed-card embed-webcard" style="display: block; width: 100%; height: 155px; max-width: 500px; margin: 10px 0px;" title="Add health check to consul service for tuggle process · fujiwara/tuggle@d31cf6e" src="https://hatenablog-parts.com/embed?url=https%3A%2F%2Fgithub.com%2Ffujiwara%2Ftuggle%2Fcommit%2Fd31cf6e293ffe4b883a4f6643353c278b424cb29" frameborder="0" scrolling="no"></iframe><cite class="hatena-citation"><a href="https://github.com/fujiwara/tuggle/commit/d31cf6e293ffe4b883a4f6643353c278b424cb29">github.com</a></cite></p>
<p> </p>
<p>このオプションと、fetch-rate オプションによる帯域制限、プライベートクラウドの flavor に帯域制限による設定で再検証することになりました。</p>
<p>50 台までは捌けることがわかったのですが、全ての終了までに 30分かかりました。本番環境では倍以上のインスタンスに配布しなければならないのでこのままではデグレになってしまいます...1.4GB の tarball は予想以上に重いようです。</p>
<h4>最後は物理で</h4>
<p>このままではいかんと考えていた時、ちょうどプライベートクラウドに SSD 搭載母艦が追加されたのを思い出しました。デプロイ先の www インスタンスはサービスの構成的に I/O はそこまで必要ないロールだったので、HDD 搭載母艦が使われていました。</p>
<p>「tarball の書き込み処理が早くなれば高速化されないだろうか？」なんの根拠も無い考えでしたが、もうアイディアも残ってなかったので思い切ってやってみました。</p>
<p>条件は同じ50台で行なった結果、全ての終了までの時間が3分になりました。そこから思い切って 100, 150 台とインスタンスを増やしましたが、どれも 3分台に収まる好成績を残しました。</p>
<p>HDD, SSD 共にベンチマークを測定したところ、SSD は HDD の 6倍の read 1.7倍の write という結果になり、2017 年ですが SSD すげぇという意識が広まりました。</p>
<p>デプロイタイムも 50% カットという驚異的な成績を残すことができました。</p>
<h4>まとめ</h4>
<p>結局、物理かよ！という感じの結論になりましたが、この調査をフィードバックした結果、SSD 搭載母艦にマイグレーションしていくという一つのきっかけになりました。配属1年目だったこともあり、全ての調査、検証が貴重な経験になりました。今回は SSD による解決になりましたが、スイッチが 10G などであればこの問題は起きないでしょう。また配布物がもっと小さい場合も起きづらい問題だと思います。</p>
<p>tuggle が無ければこのような調査や認識を持つことができなかったと思います。開発者の @fujiwara さんと Twitter で直接やりとりができたことによって解決することができました。この場を借りて、お礼申し上げます。</p>
<p>複数のインスタンスへのデプロイ時間に悩まされている方、この機会に tuggle を導入してみてはいかがでしょうか？</p>
